#!/usr/bin/env bash
set -euo pipefail
shopt -s nullglob

VERSION="1.0.0"
BASE_DIR="$HOME/Dropbox/Tersono Cloud"
SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"

# --- Colors ---
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log()  { echo -e "${GREEN}‚úÖ $1${NC}"; }
warn() { echo -e "${YELLOW}‚ö†Ô∏è  $1${NC}"; }
err()  { echo -e "${RED}‚ùå $1${NC}"; }
info() { echo -e "${BLUE}‚ÑπÔ∏è  $1${NC}"; }

show_help() {
    cat << EOF
grab v${VERSION}
Download and archive content from URLs

USAGE:
    grab <url>

SUPPORTED:
    - X/Twitter tweets (with video, images, or text-only)
    - X/Twitter articles
    - Reddit posts (text, images, video, galleries)
    - YouTube videos

OUTPUT:
    All downloads saved to: $BASE_DIR/<folder>/

REQUIREMENTS:
    brew install yt-dlp ffmpeg
    OPENAI_API_KEY env var (for transcription/summaries)

EOF
}

check_deps() {
    local missing=()
    command -v yt-dlp >/dev/null || missing+=("yt-dlp")
    command -v ffmpeg >/dev/null || missing+=("ffmpeg")
    command -v curl >/dev/null || missing+=("curl")

    if [[ ${#missing[@]} -gt 0 ]]; then
        err "Missing dependencies: ${missing[*]}"
        echo "   Install: brew install ${missing[*]}"
        exit 1
    fi
}

# --- Helpers ---

slugify() {
    echo "$1" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]/-/g' | sed 's/--*/-/g' | sed 's/^-//' | sed 's/-$//' | head -c 60
}

date_str() {
    date +%Y-%m-%d
}

transcribe_audio() {
    local audio_file="$1"
    local output_file="$2"

    if [[ -z "${OPENAI_API_KEY:-}" ]]; then
        warn "OPENAI_API_KEY not set. Skipping transcription."
        return 1
    fi

    info "Transcribing audio..."

    # Check file size ‚Äî OpenAI limit is 25MB
    local file_size
    file_size=$(stat -f%z "$audio_file" 2>/dev/null || stat -c%s "$audio_file" 2>/dev/null || echo 0)

    if [[ "$file_size" -gt 25000000 ]]; then
        info "File > 25MB, extracting audio and splitting..."
        local tmp_dir
        tmp_dir=$(mktemp -d)
        local audio_only="$tmp_dir/audio.m4a"

        # Extract audio only (much smaller)
        ffmpeg -i "$audio_file" -vn -acodec aac -b:a 64k -y "$audio_only" 2>/dev/null

        local audio_size
        audio_size=$(stat -f%z "$audio_only" 2>/dev/null || stat -c%s "$audio_only" 2>/dev/null || echo 0)

        if [[ "$audio_size" -gt 25000000 ]]; then
            # Split into chunks
            local duration
            duration=$(ffprobe -v quiet -show_entries format=duration -of csv=p=0 "$audio_only" 2>/dev/null | cut -d. -f1)
            local chunk_duration=600  # 10 min chunks
            local chunks=()
            local offset=0
            local i=0

            while [[ "$offset" -lt "$duration" ]]; do
                local chunk_file="$tmp_dir/chunk_$(printf '%03d' $i).m4a"
                ffmpeg -i "$audio_only" -ss "$offset" -t "$chunk_duration" -acodec aac -b:a 64k -y "$chunk_file" 2>/dev/null
                chunks+=("$chunk_file")
                offset=$((offset + chunk_duration))
                i=$((i + 1))
            done

            # Transcribe each chunk
            local full_transcript=""
            for chunk in "${chunks[@]}"; do
                local chunk_result
                chunk_result=$(curl -sS https://api.openai.com/v1/audio/transcriptions \
                    -H "Authorization: Bearer $OPENAI_API_KEY" \
                    -F "file=@$chunk" \
                    -F "model=whisper-1" \
                    -F "response_format=text" 2>/dev/null) || true
                full_transcript="$full_transcript $chunk_result"
            done

            echo "$full_transcript" > "$output_file"
        else
            # Small enough after audio extraction
            local result
            result=$(curl -sS https://api.openai.com/v1/audio/transcriptions \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -F "file=@$audio_only" \
                -F "model=whisper-1" \
                -F "response_format=text" 2>/dev/null) || true
            echo "$result" > "$output_file"
        fi

        rm -rf "$tmp_dir"
    else
        # Small enough, transcribe directly
        local result
        result=$(curl -sS https://api.openai.com/v1/audio/transcriptions \
            -H "Authorization: Bearer $OPENAI_API_KEY" \
            -F "file=@$audio_file" \
            -F "model=whisper-1" \
            -F "response_format=text" 2>/dev/null) || true
        echo "$result" > "$output_file"
    fi

    if [[ -s "$output_file" ]]; then
        local words
        words=$(wc -w < "$output_file" | tr -d ' ')
        log "Transcript saved ($words words)"
        return 0
    else
        warn "Transcription produced no output"
        rm -f "$output_file"
        return 1
    fi
}

summarize_text() {
    local input_file="$1"
    local output_file="$2"
    local context="${3:-video}"

    if [[ -z "${OPENAI_API_KEY:-}" ]]; then
        warn "OPENAI_API_KEY not set. Skipping summary."
        return 1
    fi

    if [[ ! -s "$input_file" ]]; then
        warn "No content to summarize"
        return 1
    fi

    info "Generating summary..."

    local content
    content=$(head -c 100000 "$input_file")

    local payload
    payload=$(GRAB_CONTEXT="$context" python3 -c "
import json, sys, os
content = sys.stdin.read()
ctx = os.environ.get('GRAB_CONTEXT', 'video')
data = {
    'model': 'gpt-4o-mini',
    'messages': [
        {'role': 'system', 'content': f'You are an expert summarizer. Given a transcript of a {ctx}, provide a thorough summary covering: 1. Main topics discussed 2. Key insights and takeaways 3. Notable quotes or moments. Format with clear sections and bullet points. Be detailed but digestible.'},
        {'role': 'user', 'content': content}
    ],
    'temperature': 0.2
}
print(json.dumps(data))
" <<< "$content")

    local response
    response=$(curl -sS https://api.openai.com/v1/chat/completions \
        -H "Authorization: Bearer $OPENAI_API_KEY" \
        -H "Content-Type: application/json" \
        -d "$payload" 2>/dev/null) || {
            warn "Summary API request failed"
            return 1
        }

    local summary
    summary=$(python3 -c "
import json, sys
try:
    data = json.load(sys.stdin)
    print(data['choices'][0]['message']['content'])
except:
    sys.exit(1)
" <<< "$response") || {
        warn "Failed to parse summary response"
        return 1
    }

    echo "$summary" > "$output_file"
    log "Summary saved"
    return 0
}

generate_title() {
    local transcript_file="$1"
    local context="${2:-video}"

    if [[ -z "${OPENAI_API_KEY:-}" ]]; then
        echo ""
        return
    fi

    if [[ ! -s "$transcript_file" ]]; then
        echo ""
        return
    fi

    local content
    content=$(head -c 10000 "$transcript_file")

    local payload
    payload=$(GRAB_CONTEXT="$context" python3 -c "
import json, sys, os
content = sys.stdin.read()
ctx = os.environ.get('GRAB_CONTEXT', 'video')
data = {
    'model': 'gpt-4o-mini',
    'messages': [
        {'role': 'system', 'content': f'Given a transcript of a {ctx}, generate a short descriptive title (5-10 words) that captures the main topic or message. Just output the title, nothing else. No quotes.'},
        {'role': 'user', 'content': content}
    ],
    'max_tokens': 30,
    'temperature': 0.3
}
print(json.dumps(data))
" <<< "$content")

    local response
    response=$(curl -sS https://api.openai.com/v1/chat/completions \
        -H "Authorization: Bearer $OPENAI_API_KEY" \
        -H "Content-Type: application/json" \
        -d "$payload" 2>/dev/null) || {
        echo ""
        return
    }

    python3 -c "
import json, sys
try:
    data = json.load(sys.stdin)
    print(data['choices'][0]['message']['content'].strip('\"').strip())
except:
    print('')
" <<< "$response"
}

describe_image() {
    local image_file="$1"

    if [[ -z "${OPENAI_API_KEY:-}" ]]; then
        echo "image"
        return
    fi

    local base64_img
    base64_img=$(base64 -i "$image_file" 2>/dev/null | tr -d '\n')

    local ext
    ext="${image_file##*.}"
    local mime="image/jpeg"
    [[ "$ext" == "png" ]] && mime="image/png"
    [[ "$ext" == "webp" ]] && mime="image/webp"
    [[ "$ext" == "gif" ]] && mime="image/gif"

    local payload
    payload=$(python3 -c "
import json
data = {
    'model': 'gpt-4o-mini',
    'messages': [
        {'role': 'user', 'content': [
            {'type': 'text', 'text': 'Describe this image in 5-8 words for use as a folder name. Be specific and descriptive. Just the description, nothing else.'},
            {'type': 'image_url', 'image_url': {'url': 'data:$mime;base64,$(echo "$base64_img" | head -c 50000)'}}
        ]}
    ],
    'max_tokens': 30
}
print(json.dumps(data))
")

    local response
    response=$(curl -sS https://api.openai.com/v1/chat/completions \
        -H "Authorization: Bearer $OPENAI_API_KEY" \
        -H "Content-Type: application/json" \
        -d "$payload" 2>/dev/null) || {
        echo "image"
        return
    }

    python3 -c "
import json, sys
try:
    data = json.load(sys.stdin)
    print(data['choices'][0]['message']['content'].strip('\"').strip())
except:
    print('image')
" <<< "$response"
}

# --- Tweet Handler ---

grab_tweet() {
    local url="$1"

    info "Fetching tweet metadata..."

    # Get tweet JSON via yt-dlp
    local json_data
    json_data=$(yt-dlp --no-update --dump-json "$url" 2>/dev/null) || true

    local has_video=false
    local has_images=false
    local tweet_text=""
    local author=""
    local author_id=""
    local upload_date=""
    local like_count=""
    local repost_count=""
    local reply_count=""
    local view_count=""
    local title=""

    if [[ -n "$json_data" ]]; then
        has_video=true
        author=$(echo "$json_data" | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('uploader','unknown'))" 2>/dev/null) || author="unknown"
        author_id=$(echo "$json_data" | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('uploader_id',''))" 2>/dev/null) || author_id=""
        tweet_text=$(echo "$json_data" | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('description',''))" 2>/dev/null) || tweet_text=""
        upload_date=$(echo "$json_data" | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('upload_date',''))" 2>/dev/null) || upload_date=""
        like_count=$(echo "$json_data" | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('like_count',''))" 2>/dev/null) || like_count=""
        repost_count=$(echo "$json_data" | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('repost_count',''))" 2>/dev/null) || repost_count=""
        view_count=$(echo "$json_data" | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('view_count',''))" 2>/dev/null) || view_count=""
        title=$(echo "$json_data" | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('title',''))" 2>/dev/null) || title=""
    fi

    # If no video, try to get tweet text and images via guest API
    if [[ "$has_video" == false ]]; then
        info "No video found, checking for text/images..."

        # Extract tweet ID from URL
        local tweet_id
        tweet_id=$(echo "$url" | grep -oE '[0-9]{15,}' | head -1)

        if [[ -n "$tweet_id" ]]; then
            # Try syndication API for text
            local synd
            synd=$(curl -sS "https://cdn.syndication.twimg.com/tweet-result?id=$tweet_id&token=0" 2>/dev/null) || true

            if [[ -n "$synd" ]]; then
                author=$(echo "$synd" | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('user',{}).get('name','unknown'))" 2>/dev/null) || author="unknown"
                author_id=$(echo "$synd" | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('user',{}).get('screen_name',''))" 2>/dev/null) || author_id=""
                tweet_text=$(echo "$synd" | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('text',''))" 2>/dev/null) || tweet_text=""
                upload_date=$(echo "$synd" | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('created_at','')[:10])" 2>/dev/null) || upload_date=""

                # Check for images
                local image_urls
                image_urls=$(echo "$synd" | python3 -c "
import sys, json
d = json.load(sys.stdin)
photos = d.get('mediaDetails', [])
for p in photos:
    if p.get('type') == 'photo':
        print(p.get('media_url_https', ''))
" 2>/dev/null) || true

                if [[ -n "$image_urls" ]]; then
                    has_images=true
                fi
            fi
        fi
    fi

    # Build folder name
    local slug=""
    if [[ -n "$tweet_text" ]]; then
        slug=$(slugify "$(echo "$tweet_text" | head -c 80)")
    elif [[ -n "$title" ]]; then
        slug=$(slugify "$title")
    fi
    [[ -z "$slug" ]] && slug="tweet"

    local sub_dir="$BASE_DIR/XPosts"
    local folder_name="$(date_str)_${slug}"
    local out_dir="$sub_dir/$folder_name"
    mkdir -p "$out_dir"

    info "Saving to: $folder_name/"

    # Save tweet text
    {
        echo "Author: $author (@${author_id})"
        echo "Date: $upload_date"
        echo "URL: $url"
        [[ -n "$like_count" ]] && echo "Likes: $like_count"
        [[ -n "$repost_count" ]] && echo "Reposts: $repost_count"
        [[ -n "$view_count" ]] && echo "Views: $view_count"
        echo ""
        echo "---"
        echo ""
        echo "$tweet_text"
    } > "$out_dir/tweet.txt"
    log "Tweet text saved"

    # Download video if present
    if [[ "$has_video" == true ]]; then
        info "Downloading video..."
        yt-dlp --no-update --no-warnings \
            -o "$out_dir/video.%(ext)s" \
            "$url" 2>/dev/null || {
                warn "Video download failed"
            }

        # Find the downloaded video file
        local video_file=""
        for f in "$out_dir"/video.*; do
            if [[ -f "$f" && "$f" != *".part" ]]; then
                video_file="$f"
                break
            fi
        done

        if [[ -n "$video_file" ]]; then
            local vsize
            vsize=$(du -h "$video_file" | cut -f1)
            log "Video saved ($vsize)"

            # Transcribe
            if transcribe_audio "$video_file" "$out_dir/transcript.txt"; then
                # Summarize
                summarize_text "$out_dir/transcript.txt" "$out_dir/summary.txt" "video from a tweet"

                # Generate descriptive title from transcript and rename folder
                info "Generating descriptive title..."
                local desc_title
                desc_title=$(generate_title "$out_dir/transcript.txt" "video from a tweet")
                if [[ -n "$desc_title" ]]; then
                    local new_slug
                    new_slug=$(slugify "$desc_title")
                    if [[ -n "$new_slug" ]]; then
                        local new_folder="$(date_str)_${new_slug}"
                        local new_dir="$sub_dir/$new_folder"
                        if [[ "$new_dir" != "$out_dir" && ! -d "$new_dir" ]]; then
                            mv "$out_dir" "$new_dir"
                            out_dir="$new_dir"
                            folder_name="$new_folder"
                            info "Renamed folder ‚Üí $folder_name/"
                        fi
                    fi
                fi
            fi
        fi
    fi

    # Download images if present
    if [[ "$has_images" == true ]]; then
        info "Downloading images..."
        local i=1
        while IFS= read -r img_url; do
            [[ -z "$img_url" ]] && continue
            local ext="jpg"
            [[ "$img_url" == *".png"* ]] && ext="png"
            local img_file="$out_dir/image_$(printf '%02d' $i).$ext"
            curl -sS -o "$img_file" "${img_url}:large" 2>/dev/null || curl -sS -o "$img_file" "$img_url" 2>/dev/null || true
            if [[ -s "$img_file" ]]; then
                log "Image $i saved"

                # If first image and no video, try to describe it for the folder name
                if [[ $i -eq 1 && "$has_video" == false ]]; then
                    info "Analyzing image for description..."
                    local desc
                    desc=$(describe_image "$img_file")
                    if [[ -n "$desc" && "$desc" != "image" ]]; then
                        local new_slug
                        new_slug=$(slugify "$desc")
                        local new_folder="$(date_str)_${new_slug}"
                        local new_dir="$sub_dir/$new_folder"
                        if [[ "$new_dir" != "$out_dir" ]]; then
                            mv "$out_dir" "$new_dir"
                            out_dir="$new_dir"
                            folder_name="$new_folder"
                            info "Renamed folder to: $folder_name/"
                        fi
                    fi
                fi
            fi
            i=$((i + 1))
        done <<< "$image_urls"
    fi

    echo ""
    log "Done! Saved to: $out_dir/"
    echo ""
    echo "üìÅ Contents:"
    ls -1 "$out_dir" | while read -r f; do
        local size
        size=$(du -h "$out_dir/$f" | cut -f1 | tr -d ' ')
        echo "   $f ($size)"
    done
}

# --- YouTube Handler ---

grab_youtube() {
    local url="$1"

    info "Fetching YouTube metadata..."

    local json_data
    json_data=$(yt-dlp --no-update --dump-json --no-download "$url" 2>/dev/null) || {
        err "Failed to fetch YouTube metadata"
        exit 1
    }

    local title
    title=$(echo "$json_data" | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('title','video'))" 2>/dev/null) || title="video"
    local uploader
    uploader=$(echo "$json_data" | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('uploader','unknown'))" 2>/dev/null) || uploader="unknown"
    local description
    description=$(echo "$json_data" | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('description',''))" 2>/dev/null) || description=""
    local thumbnail
    thumbnail=$(echo "$json_data" | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('thumbnail',''))" 2>/dev/null) || thumbnail=""
    local duration
    duration=$(echo "$json_data" | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('duration_string',''))" 2>/dev/null) || duration=""
    local view_count
    view_count=$(echo "$json_data" | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('view_count',''))" 2>/dev/null) || view_count=""
    local like_count
    like_count=$(echo "$json_data" | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('like_count',''))" 2>/dev/null) || like_count=""
    local upload_date
    upload_date=$(echo "$json_data" | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('upload_date',''))" 2>/dev/null) || upload_date=""

    local slug
    slug=$(slugify "$title")
    local sub_dir="$BASE_DIR/Youtube"
    local folder_name="$(date_str)_${slug}"
    local out_dir="$sub_dir/$folder_name"
    mkdir -p "$out_dir"

    info "Saving to: $folder_name/"

    # Save description
    {
        echo "Title: $title"
        echo "Channel: $uploader"
        echo "Date: $upload_date"
        echo "Duration: $duration"
        echo "URL: $url"
        [[ -n "$view_count" ]] && echo "Views: $view_count"
        [[ -n "$like_count" ]] && echo "Likes: $like_count"
        echo ""
        echo "---"
        echo ""
        echo "$description"
    } > "$out_dir/description.txt"
    log "Description saved"

    # Download thumbnail
    if [[ -n "$thumbnail" ]]; then
        info "Downloading thumbnail..."
        curl -sS -o "$out_dir/thumbnail.jpg" "$thumbnail" 2>/dev/null && log "Thumbnail saved" || warn "Thumbnail download failed"
    fi

    # Download video
    info "Downloading video..."
    yt-dlp --no-update --no-warnings \
        -f "bestvideo[height<=1080]+bestaudio/best[height<=1080]/best" \
        --merge-output-format mp4 \
        -o "$out_dir/video.%(ext)s" \
        "$url" 2>/dev/null || {
            warn "Video download failed"
        }

    local video_file=""
    for f in "$out_dir"/video.*; do
        if [[ -f "$f" && "$f" != *".part" ]]; then
            video_file="$f"
            break
        fi
    done

    if [[ -n "$video_file" ]]; then
        local vsize
        vsize=$(du -h "$video_file" | cut -f1)
        log "Video saved ($vsize)"

        # Transcribe
        if transcribe_audio "$video_file" "$out_dir/transcript.txt"; then
            # Summarize
            summarize_text "$out_dir/transcript.txt" "$out_dir/summary.txt" "YouTube video titled '$title'"
        fi
    fi

    echo ""
    log "Done! Saved to: $out_dir/"
    echo ""
    echo "üìÅ Contents:"
    ls -1 "$out_dir" | while read -r f; do
        local size
        size=$(du -h "$out_dir/$f" | cut -f1 | tr -d ' ')
        echo "   $f ($size)"
    done
}

# --- Reddit Handler ---

grab_reddit() {
    local url="$1"

    # Clean URL: remove query params, trailing slashes, ensure no .json yet
    local clean_url
    clean_url=$(echo "$url" | sed 's/[?#].*//' | sed 's/\/$//')

    info "Fetching Reddit post..."

    # Try JSON API first (multiple User-Agents), fall back to AGENT_FETCH signal
    local json_data=""
    local tmp_json tmp_parsed
    tmp_json=$(mktemp)
    tmp_parsed=$(mktemp)

    for ua in "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)" "grab-skill/1.0"; do
        json_data=$(curl -sS -L -H "User-Agent: $ua" "${clean_url}.json" 2>/dev/null) || continue
        # Check if it's valid JSON array (not error)
        echo "$json_data" | python3 -c "
import json, sys
d = json.load(sys.stdin)
if isinstance(d, list) and len(d) >= 1:
    sys.exit(0)
sys.exit(1)
" 2>/dev/null && break
        json_data=""
    done

    if [[ -z "$json_data" ]]; then
        # Signal the agent to handle via web_fetch
        echo "REDDIT_FETCH_NEEDED:${clean_url}"
        info "Reddit API blocked ‚Äî requires agent-level web_fetch."
        info "URL: $clean_url"
        rm -f "$tmp_json" "$tmp_parsed"
        exit 3
    fi

    echo "$json_data" > "$tmp_json"

    python3 -c "
import json, sys, html
from datetime import datetime, timezone

with open('$tmp_json') as f:
    data = json.load(f)

post = data[0]['data']['children'][0]['data']
comments = data[1]['data']['children'] if len(data) > 1 else []

title = html.unescape(post.get('title', ''))
author = post.get('author', 'unknown')
subreddit = post.get('subreddit_prefixed', '')
score = post.get('score', 0)
upvote_ratio = post.get('upvote_ratio', 0)
num_comments = post.get('num_comments', 0)
created_utc = post.get('created_utc', 0)
selftext = html.unescape(post.get('selftext', ''))
permalink = post.get('permalink', '')
post_url = post.get('url', '')
is_video = post.get('is_video', False)
domain = post.get('domain', '')

images = []
if post.get('preview', {}).get('images'):
    for img in post['preview']['images']:
        src = html.unescape(img['source']['url'])
        images.append(src)
if post.get('is_gallery') and post.get('media_metadata'):
    for mid, mdata in post['media_metadata'].items():
        if mdata.get('s', {}).get('u'):
            images.append(html.unescape(mdata['s']['u']))

top_comments = []
for c in comments[:20]:
    if c.get('kind') != 't1':
        continue
    cd = c.get('data', {})
    if not cd.get('body'):
        continue
    top_comments.append({
        'author': cd.get('author', '[deleted]'),
        'score': cd.get('score', 0),
        'body': html.unescape(cd.get('body', ''))
    })

dt = datetime.fromtimestamp(created_utc, tz=timezone.utc)
date_str = dt.strftime('%Y-%m-%d %H:%M UTC')

result = {
    'title': title, 'author': author, 'subreddit': subreddit,
    'score': score, 'upvote_ratio': upvote_ratio, 'num_comments': num_comments,
    'date': date_str, 'selftext': selftext, 'permalink': permalink,
    'post_url': post_url, 'is_video': is_video, 'domain': domain,
    'images': images, 'top_comments': top_comments
}

with open('$tmp_parsed', 'w') as f:
    json.dump(result, f)
" || {
        rm -f "$tmp_json" "$tmp_parsed"
        err "Failed to parse Reddit data"
        exit 1
    }

    # Extract fields
    local title author subreddit score num_comments date_field selftext post_url is_video domain permalink
    title=$(python3 -c "import json; d=json.load(open('$tmp_parsed')); print(d['title'])")
    author=$(python3 -c "import json; d=json.load(open('$tmp_parsed')); print(d['author'])")
    subreddit=$(python3 -c "import json; d=json.load(open('$tmp_parsed')); print(d['subreddit'])")
    score=$(python3 -c "import json; d=json.load(open('$tmp_parsed')); print(d['score'])")
    num_comments=$(python3 -c "import json; d=json.load(open('$tmp_parsed')); print(d['num_comments'])")
    date_field=$(python3 -c "import json; d=json.load(open('$tmp_parsed')); print(d['date'])")
    selftext=$(python3 -c "import json; d=json.load(open('$tmp_parsed')); print(d['selftext'])")
    post_url=$(python3 -c "import json; d=json.load(open('$tmp_parsed')); print(d['post_url'])")
    is_video=$(python3 -c "import json; d=json.load(open('$tmp_parsed')); print(d['is_video'])")
    domain=$(python3 -c "import json; d=json.load(open('$tmp_parsed')); print(d['domain'])")
    permalink=$(python3 -c "import json; d=json.load(open('$tmp_parsed')); print(d['permalink'])")

    local slug
    slug=$(slugify "$(echo "$title" | head -c 80)")
    [[ -z "$slug" ]] && slug="reddit-post"

    local sub_dir="$BASE_DIR/Reddit"
    local folder_name="$(date_str)_${slug}"
    local out_dir="$sub_dir/$folder_name"
    mkdir -p "$out_dir"

    info "Saving to: $folder_name/"

    # Save post text
    {
        echo "Title: $title"
        echo "Author: u/$author"
        echo "Subreddit: $subreddit"
        echo "Date: $date_field"
        echo "Score: $score"
        echo "Comments: $num_comments"
        echo "URL: https://reddit.com${permalink}"
        [[ -n "$post_url" && "$domain" != "self."* ]] && echo "Link: $post_url"
        echo ""
        echo "---"
        echo ""
        echo "$selftext"
    } > "$out_dir/post.txt"
    log "Post saved"

    # Save top comments
    python3 -c "
import json
data = json.load(open('$tmp_parsed'))
comments = data['top_comments']
if not comments:
    exit(0)
for i, c in enumerate(comments, 1):
    print(f\"--- Comment {i} (u/{c['author']}, {c['score']} points) ---\")
    print(c['body'])
    print()
" > "$out_dir/comments.txt" 2>/dev/null
    if [[ -s "$out_dir/comments.txt" ]]; then
        local ccount
        ccount=$(grep -c "^--- Comment" "$out_dir/comments.txt" || echo 0)
        log "Top $ccount comments saved"
    else
        rm -f "$out_dir/comments.txt"
    fi

    # Download images
    local image_count
    image_count=$(python3 -c "import json; print(len(json.load(open('$tmp_parsed'))['images']))")
    if [[ "$image_count" -gt 0 ]]; then
        info "Downloading $image_count image(s)..."
        IMG_IDX=1
        python3 -c "
import json
data = json.load(open('$tmp_parsed'))
for url in data['images']:
    print(url)
" | while IFS= read -r img_url; do
            [[ -z "$img_url" ]] && continue
            local idx="${IMG_IDX:-1}"
            IMG_IDX=$((idx + 1))
            curl -sS -o "$out_dir/image_$(printf '%02d' $idx).jpg" "$img_url" 2>/dev/null && \
                log "Image $idx saved" || warn "Image $idx failed"
        done
    fi

    # Download video if present
    if [[ "$is_video" == "True" ]]; then
        info "Downloading video..."
        yt-dlp --no-update --no-warnings \
            -o "$out_dir/video.%(ext)s" \
            "$clean_url" 2>/dev/null || warn "Video download failed"

        local video_file=""
        for f in "$out_dir"/video.*; do
            if [[ -f "$f" && "$f" != *".part" ]]; then
                video_file="$f"
                break
            fi
        done

        if [[ -n "$video_file" ]]; then
            local vsize
            vsize=$(du -h "$video_file" | cut -f1)
            log "Video saved ($vsize)"

            if transcribe_audio "$video_file" "$out_dir/transcript.txt"; then
                summarize_text "$out_dir/transcript.txt" "$out_dir/summary.txt" "Reddit video post titled '$title'"
            fi
        fi
    fi

    # Generate summary from post text + comments if substantial
    if [[ -z "$is_video" || "$is_video" == "False" ]]; then
        local combined_size=0
        [[ -s "$out_dir/post.txt" ]] && combined_size=$((combined_size + $(wc -c < "$out_dir/post.txt")))
        [[ -s "$out_dir/comments.txt" ]] && combined_size=$((combined_size + $(wc -c < "$out_dir/comments.txt")))

        if [[ "$combined_size" -gt 500 ]]; then
            local combined_file
            combined_file=$(mktemp)
            cat "$out_dir/post.txt" > "$combined_file"
            [[ -s "$out_dir/comments.txt" ]] && { echo -e "\n\n=== TOP COMMENTS ===\n"; cat "$out_dir/comments.txt"; } >> "$combined_file"
            summarize_text "$combined_file" "$out_dir/summary.txt" "Reddit post and discussion titled '$title' in $subreddit"
            rm -f "$combined_file"
        fi
    fi

    # Cleanup temp files
    rm -f "$tmp_json" "$tmp_parsed"

    echo ""
    log "Done! Saved to: $out_dir/"
    echo ""
    echo "üìÅ Contents:"
    ls -1 "$out_dir" | while read -r f; do
        local size
        size=$(du -h "$out_dir/$f" | cut -f1 | tr -d ' ')
        echo "   $f ($size)"
    done
}

# --- Main ---

URL="${1:-}"

if [[ -z "$URL" || "$URL" == "--help" || "$URL" == "-h" ]]; then
    show_help
    exit 0
fi

if [[ "$URL" == "--version" || "$URL" == "-v" ]]; then
    echo "grab v${VERSION}"
    exit 0
fi

check_deps
mkdir -p "$BASE_DIR"

# Detect URL type ‚Äî check for X articles first
if [[ "$URL" =~ (x\.com|twitter\.com)/.*/status/ ]]; then
    # Check if this tweet is actually an article
    article_check=$(yt-dlp --no-update --dump-json --no-download "$URL" 2>&1 || true)
    if echo "$article_check" | grep -q "x\.com/i/article/"; then
        article_id=$(echo "$article_check" | grep -oE 'article/[0-9]+' | head -1 | cut -d/ -f2)
        echo "ARTICLE_DETECTED:${article_id}:${URL}"
        info "This is an X Article ‚Äî requires browser to extract content."
        info "Article ID: $article_id"
        exit 2
    fi
    grab_tweet "$URL"
elif [[ "$URL" =~ (youtube\.com|youtu\.be) ]]; then
    grab_youtube "$URL"
elif [[ "$URL" =~ (reddit\.com|redd\.it) ]]; then
    grab_reddit "$URL"
else
    # Try generic yt-dlp download
    info "Unknown URL type, attempting generic download..."
    SLUG=$(slugify "$(echo "$URL" | sed 's|https\?://||' | head -c 60)")
    FOLDER_NAME="download_${SLUG}_$(date_str)"
    OUT_DIR="$BASE_DIR/$FOLDER_NAME"
    mkdir -p "$OUT_DIR"

    yt-dlp --no-update --no-warnings \
        -o "$OUT_DIR/video.%(ext)s" \
        "$URL" 2>/dev/null && log "Downloaded to: $FOLDER_NAME/" || err "Download failed"
fi
